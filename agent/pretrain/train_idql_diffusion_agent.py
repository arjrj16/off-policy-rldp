"""
Offline IDQL pre-training for diffusion policy.

This is the offline version of agent/finetune/train_idql_diffusion_agent.py.
Instead of collecting data online via environment rollouts, it uses an 
offline dataset with rewards (generated by relabel_robomimic_rewards.py).

The training loop is identical to finetuning:
1. Critic V: expectile regression against Q
2. Critic Q: Bellman backup with rewards  
3. Actor: Behavior cloning loss

This allows Q and V functions to be pre-trained offline before finetuning,
giving better initialization than random Q/V networks.
"""

import os
import pickle
import numpy as np
import torch
import logging
import wandb
from copy import deepcopy

log = logging.getLogger(__name__)
from util.timer import Timer
from agent.pretrain.train_agent import PreTrainAgent, batch_to_device
from util.scheduler import CosineAnnealingWarmupRestarts


class TrainIDQLDiffusionAgent(PreTrainAgent):
    """
    Offline IDQL pretraining agent.
    
    Mirrors the finetune TrainIDQLDiffusionAgent but uses offline dataset
    instead of online rollouts.
    """

    def __init__(self, cfg):
        # Don't call super().__init__ yet - we need to set up optimizers differently
        # Manually do what PreTrainAgent.__init__ does, but skip optimizer setup
        import random
        import hydra
        from omegaconf import OmegaConf
        
        self.cfg = cfg
        self.seed = cfg.get("seed", 42)
        random.seed(self.seed)
        np.random.seed(self.seed)
        torch.manual_seed(self.seed)
        self.device = cfg.device

        # Wandb
        self.use_wandb = cfg.wandb is not None
        if cfg.wandb is not None:
            wandb.init(
                entity=cfg.wandb.entity,
                project=cfg.wandb.project,
                name=cfg.wandb.run,
                config=OmegaConf.to_container(cfg, resolve=True),
            )

        # Build model (IDQLDiffusion with actor, critic_q, critic_v)
        self.model = hydra.utils.instantiate(cfg.model)
        
        # EMA for actor only (same as PreTrainAgent)
        from agent.pretrain.train_agent import EMA
        self.ema = EMA(cfg.ema)
        self.ema_model = deepcopy(self.model)

        # Training params
        self.n_epochs = cfg.train.n_epochs
        self.batch_size = cfg.train.batch_size
        self.epoch_start_ema = cfg.train.get("epoch_start_ema", 20)
        self.update_ema_freq = cfg.train.get("update_ema_freq", 10)
        self.val_freq = cfg.train.get("val_freq", 100)

        # Logging, checkpoints
        self.logdir = cfg.logdir
        self.checkpoint_dir = os.path.join(self.logdir, "checkpoint")
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        self.log_freq = cfg.train.get("log_freq", 1)
        self.save_model_freq = cfg.train.save_model_freq

        # Build dataset (StitchedSequenceQLearningDataset with rewards)
        self.dataset_train = hydra.utils.instantiate(cfg.train_dataset)
        self.dataloader_train = torch.utils.data.DataLoader(
            self.dataset_train,
            batch_size=self.batch_size,
            num_workers=4 if self.dataset_train.device == "cpu" else 0,
            shuffle=True,
            pin_memory=True if self.dataset_train.device == "cpu" else False,
        )

        # ========== IDQL-specific setup (mirrors finetune) ==========
        
        # Discount factor
        self.gamma = cfg.train.gamma
        
        # Critic warmup (train critics for N epochs before updating actor)
        self.n_critic_warmup_epochs = cfg.train.get("n_critic_warmup_epochs", 0)
        
        # Reward scaling
        self.scale_reward_factor = cfg.train.get("scale_reward_factor", 1.0)
        
        # Target network update rate
        self.critic_tau = cfg.train.get("critic_tau", 0.005)
        
        # Gradient clipping
        self.max_grad_norm = cfg.train.get("max_grad_norm", None)
        
        # Action steps (for Q function input)
        self.act_steps = cfg.get("act_steps", cfg.horizon_steps)

        # ========== Optimizers (same structure as finetune) ==========
        
        self.actor_optimizer = torch.optim.AdamW(
            self.model.actor.parameters(),
            lr=cfg.train.actor_lr,
            weight_decay=cfg.train.get("actor_weight_decay", 1e-6),
        )
        self.actor_lr_scheduler = CosineAnnealingWarmupRestarts(
            self.actor_optimizer,
            first_cycle_steps=cfg.train.actor_lr_scheduler.first_cycle_steps,
            cycle_mult=1.0,
            max_lr=cfg.train.actor_lr,
            min_lr=cfg.train.actor_lr_scheduler.min_lr,
            warmup_steps=cfg.train.actor_lr_scheduler.warmup_steps,
            gamma=1.0,
        )
        
        self.critic_q_optimizer = torch.optim.AdamW(
            self.model.critic_q.parameters(),
            lr=cfg.train.critic_lr,
            weight_decay=cfg.train.get("critic_weight_decay", 0),
        )
        self.critic_q_lr_scheduler = CosineAnnealingWarmupRestarts(
            self.critic_q_optimizer,
            first_cycle_steps=cfg.train.critic_lr_scheduler.first_cycle_steps,
            cycle_mult=1.0,
            max_lr=cfg.train.critic_lr,
            min_lr=cfg.train.critic_lr_scheduler.min_lr,
            warmup_steps=cfg.train.critic_lr_scheduler.warmup_steps,
            gamma=1.0,
        )
        
        self.critic_v_optimizer = torch.optim.AdamW(
            self.model.critic_v.parameters(),
            lr=cfg.train.critic_lr,
            weight_decay=cfg.train.get("critic_weight_decay", 0),
        )
        self.critic_v_lr_scheduler = CosineAnnealingWarmupRestarts(
            self.critic_v_optimizer,
            first_cycle_steps=cfg.train.critic_lr_scheduler.first_cycle_steps,
            cycle_mult=1.0,
            max_lr=cfg.train.critic_lr,
            min_lr=cfg.train.critic_lr_scheduler.min_lr,
            warmup_steps=cfg.train.critic_lr_scheduler.warmup_steps,
            gamma=1.0,
        )
        
        log.info(f"Offline IDQL pretraining initialized")
        log.info(f"  gamma={self.gamma}, critic_tau={self.critic_tau}")
        log.info(f"  actor_lr={cfg.train.actor_lr}, critic_lr={cfg.train.critic_lr}")
        log.info(f"  n_critic_warmup_epochs={self.n_critic_warmup_epochs}")
        log.info(f"  scale_reward_factor={self.scale_reward_factor}")

    def reset_parameters(self):
        self.ema_model.load_state_dict(self.model.state_dict())

    def step_ema(self):
        if self.epoch < self.epoch_start_ema:
            self.reset_parameters()
            return
        self.ema.update_model_average(self.ema_model, self.model)

    def run(self):
        """
        Main training loop - mirrors finetune but uses offline dataset.
        """
        timer = Timer()
        self.epoch = 1
        cnt_batch = 0
        run_results = []
        
        # Initialize EMA
        self.reset_parameters()
        
        for _ in range(self.n_epochs):
            # Track losses for logging
            loss_actor_epoch = []
            loss_critic_q_epoch = []
            loss_critic_v_epoch = []
            
            self.model.train()
            
            for batch_train in self.dataloader_train:
                if self.dataset_train.device == "cpu":
                    batch_train = batch_to_device(batch_train)
                
                # ========== Extract batch data ==========
                # batch_train is a Transition namedtuple: (actions, conditions, rewards, dones)
                # conditions has "state" and "next_state"
                actions = batch_train.actions  # (B, H, A)
                conditions = batch_train.conditions
                rewards = batch_train.rewards.view(-1) * self.scale_reward_factor  # (B,)
                dones = batch_train.dones.view(-1)  # (B,) - terminals
                
                obs_b = conditions["state"]  # (B, T, D)
                next_obs_b = conditions["next_state"]  # (B, T, D)
                
                # For Q function, only use first act_steps of actions
                actions_b = actions[:, :self.act_steps, :]  # (B, act_steps, A)
                
                # For offline data, we treat all non-terminal transitions as not truncated
                # (the dataset already handles truncation in make_indices)
                terminated_b = dones
                truncated_b = torch.zeros_like(dones)
                
                # ========== Update critic V (same as finetune lines 477-482) ==========
                critic_loss_v = self.model.loss_critic_v(
                    {"state": obs_b}, actions_b
                )
                self.critic_v_optimizer.zero_grad()
                critic_loss_v.backward()
                self.critic_v_optimizer.step()
                loss_critic_v_epoch.append(critic_loss_v.item())
                
                # ========== Update critic Q (same as finetune lines 485-496) ==========
                critic_loss_q = self.model.loss_critic_q(
                    {"state": obs_b},
                    {"state": next_obs_b},
                    actions_b,
                    rewards,
                    terminated_b,
                    truncated_b,
                    self.gamma,
                )
                self.critic_q_optimizer.zero_grad()
                critic_loss_q.backward()
                self.critic_q_optimizer.step()
                loss_critic_q_epoch.append(critic_loss_q.item())
                
                # ========== Update target Q (same as finetune line 499) ==========
                self.model.update_target_critic(self.critic_tau)
                
                # ========== Update actor (same as finetune lines 503-514) ==========
                # Use full action sequence for BC loss
                loss_actor = self.model.loss(
                    actions,  # Full horizon for BC
                    {"state": obs_b},
                )
                self.actor_optimizer.zero_grad()
                loss_actor.backward()
                
                # Only update actor after critic warmup
                if self.epoch >= self.n_critic_warmup_epochs:
                    if self.max_grad_norm is not None:
                        torch.nn.utils.clip_grad_norm_(
                            self.model.actor.parameters(), self.max_grad_norm
                        )
                    self.actor_optimizer.step()
                loss_actor_epoch.append(loss_actor.item())
                
                # Update EMA
                if cnt_batch % self.update_ema_freq == 0:
                    self.step_ema()
                cnt_batch += 1
            
            # ========== End of epoch ==========
            
            # Compute epoch losses
            loss_actor = np.mean(loss_actor_epoch)
            loss_critic_q = np.mean(loss_critic_q_epoch)
            loss_critic_v = np.mean(loss_critic_v_epoch)
            
            # Update learning rates
            self.actor_lr_scheduler.step()
            self.critic_q_lr_scheduler.step()
            self.critic_v_lr_scheduler.step()
            
            # Save model
            if self.epoch % self.save_model_freq == 0 or self.epoch == self.n_epochs:
                self.save_model()
            
            # Log
            run_results.append({
                "epoch": self.epoch,
                "loss_actor": loss_actor,
                "loss_critic_q": loss_critic_q,
                "loss_critic_v": loss_critic_v,
            })
            
            if self.epoch % self.log_freq == 0:
                time = timer()
                log.info(
                    f"{self.epoch}: actor {loss_actor:8.4f} | Q {loss_critic_q:8.4f} | V {loss_critic_v:8.4f} | t:{time:8.4f}"
                )
                if self.use_wandb:
                    wandb.log(
                        {
                            "loss - actor": loss_actor,
                            "loss - critic_q": loss_critic_q,
                            "loss - critic_v": loss_critic_v,
                        },
                        step=self.epoch,
                        commit=True,
                    )
            
            self.epoch += 1
        
        # Save final results
        result_path = os.path.join(self.logdir, "result.pkl")
        with open(result_path, "wb") as f:
            pickle.dump(run_results, f)
        log.info(f"Saved results to {result_path}")

    def save_model(self):
        """
        Saves model (actor + critics) and EMA to disk.
        """
        data = {
            "epoch": self.epoch,
            "model": self.model.state_dict(),
            "ema": self.ema_model.state_dict(),
        }
        savepath = os.path.join(self.checkpoint_dir, f"state_{self.epoch}.pt")
        torch.save(data, savepath)
        log.info(f"Saved model to {savepath}")

    def load(self, epoch):
        """
        Loads model and EMA from disk.
        """
        loadpath = os.path.join(self.checkpoint_dir, f"state_{epoch}.pt")
        data = torch.load(loadpath, weights_only=True)
        self.epoch = data["epoch"]
        self.model.load_state_dict(data["model"])
        self.ema_model.load_state_dict(data["ema"])
        log.info(f"Loaded model from {loadpath}")
